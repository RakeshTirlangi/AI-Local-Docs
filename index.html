<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Local AI File System Controller</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
        }
        h1, h2, h3, h4 {
            color: #2c3e50;
            margin-top: 1.5em;
        }
        h1 {
            font-size: 2.2em;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            font-size: 1.8em;
            border-bottom: 1px solid #bdc3c7;
            padding-bottom: 5px;
        }
        h3 {
            font-size: 1.4em;
            color: #3498db;
        }
        h4 {
            font-size: 1.2em;
        }
        code {
            background-color: #f0f0f0;
            padding: 2px 4px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        pre {
            background-color: #f8f8f8;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            border: 1px solid #ddd;
        }
        .code-block {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 20px 0;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            padding: 12px 15px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        th {
            background-color: #3498db;
            color: white;
        }
        tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        .flowchart {
            text-align: center;
            background-color: #f8f8f8;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
            border: 1px solid #ddd;
        }
        .flowchart img {
            max-width: 100%;
        }
        .example-interaction {
            background-color: #eaf2f8;
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            border-left: 5px solid #3498db;
        }
        .divider {
            border-top: 1px solid #ddd;
            margin: 30px 0;
        }
        .highlight {
            background-color: #fff3cd;
            padding: 2px 4px;
            border-radius: 3px;
        }
        .key-point {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            border-left: 5px solid #2980b9;
        }
    </style>
</head>
<body>
    <h1>Local AI File System Controller</h1>
    <p>A Local AI File System Controller that allows users to control their file system using natural language commands via a command line interface (CLI). It works completely offline using a large language model (LLM) and a modular tool execution server.</p>

    <div class="divider"></div>

    <h2>1. Overview</h2>

    <h3>Objective</h3>
    <p>Build a system that:</p>
    <ul>
        <li>Accepts natural language commands via CLI.</li>
        <li>Processes the input using a local LLM.</li>
        <li>Uses a tool-calling layer (MCP server) to perform file system actions.</li>
    </ul>

    <h3>Core Components</h3>
    <ul>
        <li>CLI Interface</li>
        <li>Local LLM (quantized, runs via Ollama)</li>
        <li>MCP Server (handles tool execution)</li>
        <li>File System (performs the real operations)</li>
    </ul>

    <div class="divider"></div>

    <h2>2. System Architecture</h2>

    <h3>Flow</h3>
    <ol>
        <li>User enters a query into the CLI.</li>
        <li>The query is passed to the local LLM.</li>
        <li>The LLM selects a tool to use.</li>
        <li>The selected tool is called via the MCP server.</li>
        <li>The tool executes the action on the file system.</li>
        <li>Output is sent back to the CLI.</li>
    </ol>

    <div class="flowchart">
        <div style="display: flex; flex-direction: row; justify-content: center; align-items: center; flex-wrap: wrap; margin: 20px 0;">
            <!-- User box -->
            <div style="display: flex; flex-direction: column; align-items: center; margin: 10px;">
                <div style="width: 120px; height: 60px; background-color: #3498db; color: white; border-radius: 8px; display: flex; justify-content: center; align-items: center; font-weight: bold;">
                    User (CLI)
                </div>
                <!-- Arrow down -->
                <div style="width: 0; height: 0; border-left: 10px solid transparent; border-right: 10px solid transparent; border-top: 15px solid #3498db; margin: 5px 0;"></div>
            </div>

            <!-- MCP Host box -->
            <div style="display: flex; flex-direction: column; align-items: center; margin: 10px;">
                <div style="width: 140px; height: 60px; background-color: #2ecc71; color: white; border-radius: 8px; display: flex; justify-content: center; align-items: center; font-weight: bold; text-align: center;">
                    MCP Host<br>(CLI)
                </div>
                <!-- Arrow down -->
                <div style="width: 0; height: 0; border-left: 10px solid transparent; border-right: 10px solid transparent; border-top: 15px solid #2ecc71; margin: 5px 0;"></div>
            </div>

            <!-- Local LLM box -->
            <div style="display: flex; flex-direction: column; align-items: center; margin: 10px;">
                <div style="width: 160px; height: 60px; background-color: #e74c3c; color: white; border-radius: 8px; display: flex; justify-content: center; align-items: center; font-weight: bold; text-align: center;">
                    Local LLM<br>(Ollama)
                </div>
                <!-- Arrow down -->
                <div style="width: 0; height: 0; border-left: 10px solid transparent; border-right: 10px solid transparent; border-top: 15px solid #e74c3c; margin: 5px 0;"></div>
            </div>

            <!-- MCP Server box -->
            <div style="display: flex; flex-direction: column; align-items: center; margin: 10px;">
                <div style="width: 160px; height: 60px; background-color: #f39c12; color: white; border-radius: 8px; display: flex; justify-content: center; align-items: center; font-weight: bold; text-align: center;">
                    MCP Server<br>(Tool APIs)
                </div>
                <!-- Arrow down -->
                <div style="width: 0; height: 0; border-left: 10px solid transparent; border-right: 10px solid transparent; border-top: 15px solid #f39c12; margin: 5px 0;"></div>
            </div>

            <!-- File System box -->
            <div style="display: flex; flex-direction: column; align-items: center; margin: 10px;">
                <div style="width: 140px; height: 60px; background-color: #9b59b6; color: white; border-radius: 8px; display: flex; justify-content: center; align-items: center; font-weight: bold; text-align: center;">
                    Local File<br>System
                </div>
            </div>
        </div>
        <p><i>System Architecture Flowchart: Data flows from User to File System through intermediary components</i></p>
    </div>

    <div class="divider"></div>

    <h2>3. LLM and Tool Calling</h2>

    <h3>Model</h3>
    <ul>
        <li>Use a <strong>small quantized model</strong> (e.g., mistral, phi, tinyllama).</li>
        <li>Serve using <strong>Ollama</strong> or directly via llama.cpp.</li>
    </ul>

    <h3>Why Quantized?</h3>
    <ul>
        <li>Fast on CPUs</li>
        <li>Lightweight and offline</li>
        <li>Efficient for simple tool selection</li>
    </ul>

    <h3>Tool Descriptions</h3>
    <p>Each tool should have:</p>
    <ul>
        <li>name</li>
        <li>description</li>
        <li>parameters</li>
    </ul>

    <h4>Example:</h4>
    <pre><code>{
  "name": "delete_files",
  "description": "Deletes files from a directory based on extension.",
  "parameters": {
    "directory": "Path to folder",
    "extension": "e.g. .txt, .log"
  }
}</code></pre>

    <p>These are fed to the LLM as context so it can choose which tool to invoke.</p>

    <div class="divider"></div>

    <h2>How to Configure the File</h2>

    <p>This configuration file defines the settings needed to launch different MCP servers (tools) on your local system. Each tool is listed under mcpServers, with its associated startup command and arguments.</p>

    <h3>JSON Structure</h3>
    <pre><code>{
  "mcpServers": {
    "filesystem": {
      "command": "cmd",
      "args": [
        "/c",
        "C:\\Users\\Rakes\\OneDrive\\Documents\\Local AI Agent\\mcp-filesystem-server\\mcp-filesystem-server.exe",
        "C:\\Users\\Rakes\\OneDrive\\Documents\\mcp"
      ]
    },
    "code-runner": {
      "command": "npx",
      "args": [
        "-y",
        "mcp-server-code-runner@latest"
      ]
    }
  }
}</code></pre>

    <h3>Explanation of Keys</h3>

    <h4>ðŸ”¸ mcpServers</h4>
    <p>This is the root object where each key (e.g., filesystem, code-runner) represents an individual MCP tool/server that the LLM can interact with.</p>

    <div class="key-point">
        <h4>ðŸ”¸ filesystem</h4>
        <p>This server allows the LLM to interact with your file system using predefined commands such as reading, writing, listing, or deleting files.</p>

        <p><strong>- command: "cmd"</strong></p>
        <p>Indicates that the server should be started using the Windows Command Prompt.</p>

        <p><strong>- args</strong></p>
        <p>Arguments passed to the command line:</p>
        <ul>
            <li>"/c" â€” Runs the command and then terminates.</li>
            <li>"C:\\...\\mcp-filesystem-server.exe" â€” Path to the executable file for the filesystem MCP server.</li>
            <li>"C:\\...\\mcp" â€” The root directory the server will have access to (for file read/write operations).</li>
        </ul>
    </div>

    <div class="key-point">
        <h4>ðŸ”¸ code-runner</h4>
        <p>This server is responsible for running code snippets passed from the LLM.</p>

        <p><strong>- command: "npx"</strong></p>
        <p>Uses npx (a Node.js package runner) to launch a code-execution server.</p>

        <p><strong>- args</strong></p>
        <p>Arguments passed to npx:</p>
        <ul>
            <li>"-y" â€” Automatically says "yes" to all prompts (useful for automation).</li>
            <li>"mcp-server-code-runner@latest" â€” Pulls and runs the latest version of the mcp-server-code-runner from the npm registry.</li>
        </ul>
    </div>

    <h3>Notes</h3>
    <ul>
        <li>All paths must be absolute and properly escaped (\\ for Windows).</li>
        <li>The command and args should form a complete valid system command.</li>
        <li>The filesystem server only has access to the directory passed as the last argument.</li>
        <li>Additional servers can be added under mcpServers following the same structure.</li>
    </ul>

    <div class="divider"></div>

    <h2>4. MCP Server and Tools</h2>

    <p>The MCP Server is a local API server that exposes tools as callable endpoints.</p>

    <h3>Example Tools</h3>
    <table>
        <thead>
            <tr>
                <th>Tool Name</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>list_files</td>
                <td>Lists files in a given directory</td>
            </tr>
            <tr>
                <td>delete_files</td>
                <td>Deletes files by extension</td>
            </tr>
            <tr>
                <td>move_file</td>
                <td>Moves a file or group of files</td>
            </tr>
            <tr>
                <td>rename_file</td>
                <td>Renames a file</td>
            </tr>
            <tr>
                <td>create_folder</td>
                <td>Creates a new directory</td>
            </tr>
        </tbody>
    </table>

    <div class="divider"></div>

    <h2>5. Execution of the system</h2>

    <h3>ðŸš€ Starting the Server</h3>

    <p>To start the server with a specific model and connect to the filesystem tools:</p>

    <pre><code>mcphost -m ollama:qwen3:1.7b</code></pre>

    <p><strong>Expected Output:</strong></p>
    <pre><code>INFO Model loaded provider=ollama model=qwen3:1.7b 
INFO Initializing server... name=filesystem 
INFO Server connected name=filesystem 
INFO Tools loaded server=filesystem count=11</code></pre>

    <h3>Example Interactions</h3>

    <div class="example-interaction">
        <h4>1. List All Folders</h4>
        <p><strong>Command:</strong></p>
        <pre><code>now list all the folders?</code></pre>

        <p><strong>Response:</strong></p>
        <ul>
            <li>Files:
                <ul>
                    <li>client1.py</li>
                    <li>gemini_client.rtf</li>
                    <li>notes.rtf</li>
                </ul>
            </li>
            <li>Folders:
                <ul>
                    <li>lnikedin_mcp</li>
                    <li>mcp-server-yt</li>
                    <li>mcp1</li>
                    <li>mcp_js</li>
                </ul>
            </li>
        </ul>
    </div>

    <div class="example-interaction">
        <h4>2. Create JavaScript File</h4>
        <p><strong>Command:</strong></p>
        <pre><code>write a file named hg.js and add "console.log("Hekk") in the file and add it to the root server directory</code></pre>

        <p><strong>Action Taken:</strong></p>
        <ul>
            <li>Created hg.js with:</li>
        </ul>
        <pre><code>console.log("Hekk");</code></pre>
    </div>

    <div class="example-interaction">
        <h4>3. Remove & Add a Python File</h4>
        <p><strong>Command:</strong></p>
        <pre><code>remove the file you just added, and add the file named prime.py and add the checking prime number code in it and save it to the root directory</code></pre>

        <p><strong>Actions Taken:</strong></p>
        <ul>
            <li>Deleted hg.js (emptied the file)</li>
            <li>Created prime.py with content:</li>
        </ul>
        <pre><code>def is_prime(n):
    if n <= 1:
        return False
    if n <= 3:
        return True
    if n % 2 == 0 or n % 3 == 0:
        return False
    
    i = 5
    while i * i <= n:
        if n % i == 0 or n % (i + 2) == 0:
            return False
        i += 6

    return True

# Example usage
number = int(input("Enter a number: "))
if is_prime(number):
    print(f"{number} is a prime number.")
else:
    print(f"{number} is not a prime number.")</code></pre>
    </div>

    <h3>Assistant Interaction Style</h3>
    <ul>
        <li>Understands natural language</li>
        <li>Clarifies ambiguous instructions</li>
        <li>Lists available functions or directories</li>
        <li>Friendly and context-aware responses</li>
    </ul>

    <p>You now have a fully functional, natural-language interface for managing your <strong>file system</strong> using a local LLM via mcphost. All interactions are:</p>
    <ul>
        <li>Local-first</li>
        <li>Smart & tool-aware</li>
        <li>Extensible with custom tools</li>
    </ul>

    <div class="divider"></div>

    <h2>6. Conclusion</h2>

    <p>This project demonstrates a powerful way to interact with local system resources using natural language through a modular MCP (Model Context Protocol) server architecture. By combining a local LLM with MCP-compatible tools like the filesystem and code-runner servers, users can seamlessly perform tasks such as file management and code execution via simple natural language commands.</p>

    <p>With a clear configuration setup and a scalable design, this system lays the groundwork for building intelligent, extensible local AI agents that can automate complex workflows and integrate additional tools with ease.</p>

</body>
</html>